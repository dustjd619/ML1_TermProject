{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 샘플링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# 정답 라벨 생성\n",
    "df[\"label\"] = (df[\"wildfire_count\"] > 0).astype(int)\n",
    "\n",
    "# 추가 feature 생성\n",
    "\n",
    "# VPD (증기압 결핍량)\n",
    "df[\"VPD\"] = (\n",
    "    0.6108\n",
    "    * np.exp((17.27 * df[\"temp\"]) / (df[\"temp\"] + 237.3))\n",
    "    * (1 - df[\"humidity\"] / 100)\n",
    ")\n",
    "\n",
    "# rain_presence 컬럼 생성\n",
    "df[\"rain_presence\"] = (df[\"rain_indicator\"] == 0).astype(int)\n",
    "\n",
    "# datetime 정렬 및 누적 계산\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df = df.sort_values([\"region\", \"datetime\"])\n",
    "\n",
    "# 7일(=168시간) rolling 합계\n",
    "df[\"rain_presence_7day_sum\"] = df.groupby(\"region\")[\"rain_presence\"].transform(\n",
    "    lambda x: x.rolling(window=168, min_periods=1).sum()\n",
    ")\n",
    "\n",
    "# 학습용 컬럼 선택\n",
    "feature_cols = [\"temp\", \"wind\", \"humidity\", \"VPD\", \"rain_presence_7day_sum\"]\n",
    "X = df[feature_cols]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 샘플링\n",
    "under = RandomUnderSampler(sampling_strategy={0: 10000}, random_state=42)\n",
    "over = BorderlineSMOTE(sampling_strategy={1: 10000}, random_state=42, k_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 모델별 파이프라인 - 하이퍼파라미터 튜닝 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\n",
    "                \"clf\",\n",
    "                RandomForestClassifier(\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"XGBoost\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\n",
    "                \"clf\",\n",
    "                XGBClassifier(\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\"clf\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "        ]\n",
    "    ),\n",
    "    \"GradientBoosting\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\"gb\", GradientBoostingClassifier(random_state=42)),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 튜닝 전 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM ---\n",
      "balanced_accuracy   : 0.7464 ± 0.0144\n",
      "f1_macro            : 0.4700 ± 0.0015\n",
      "precision_macro     : 0.5009 ± 0.0001\n",
      "recall_macro        : 0.7464 ± 0.0144\n",
      "\n",
      "--- RandomForest ---\n",
      "balanced_accuracy   : 0.6834 ± 0.0108\n",
      "f1_macro            : 0.4770 ± 0.0004\n",
      "precision_macro     : 0.5008 ± 0.0000\n",
      "recall_macro        : 0.6834 ± 0.0108\n",
      "\n",
      "--- XGBoost ---\n",
      "balanced_accuracy   : 0.7297 ± 0.0189\n",
      "f1_macro            : 0.4680 ± 0.0014\n",
      "precision_macro     : 0.5008 ± 0.0001\n",
      "recall_macro        : 0.7297 ± 0.0189\n",
      "\n",
      "--- GradientBoosting ---\n",
      "balanced_accuracy   : 0.7853 ± 0.0028\n",
      "f1_macro            : 0.4446 ± 0.0022\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.7853 ± 0.0028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# 모델별 최적 k 딕셔너리 (위 결과 기반)\n",
    "best_k_dict = {\n",
    "    \"LightGBM\": 10,\n",
    "    \"RandomForest\": 5,\n",
    "    \"XGBoost\": 10,\n",
    "    \"GradientBoosting\": 5,\n",
    "}\n",
    "\n",
    "# 최적 k 기준으로 다시 교차 검증\n",
    "for model_name, k in best_k_dict.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    pipe = models[model_name]\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "        mean = scores[f\"test_{metric}\"].mean()\n",
    "        std = scores[f\"test_{metric}\"].std()\n",
    "        print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 랜덤포레스트 test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RandomForest ---\n",
      "balanced_accuracy   : 0.7925 ± 0.0032\n",
      "f1_macro            : 0.4352 ± 0.0046\n",
      "precision_macro     : 0.5006 ± 0.0000\n",
      "recall_macro        : 0.7925 ± 0.0032\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "from imblearn.pipeline import Pipeline\n",
    "\n",
    "rf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=3,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=4,\n",
    "                max_features=\"sqrt\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_rf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores_rf = cross_validate(\n",
    "    rf_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_rf,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- RandomForest ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_rf[f\"test_{metric}\"].mean()\n",
    "    std = scores_rf[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XGBoost test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost ---\n",
      "balanced_accuracy   : 0.7928 ± 0.0133\n",
      "f1_macro            : 0.4343 ± 0.0018\n",
      "precision_macro     : 0.5006 ± 0.0000\n",
      "recall_macro        : 0.7928 ± 0.0133\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.005,\n",
    "                max_depth=3,\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.9,\n",
    "                eval_metric=\"auc\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_xgb = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores_xgb = cross_validate(\n",
    "    xgb_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_xgb,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_xgb[f\"test_{metric}\"].mean()\n",
    "    std = scores_xgb[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LightGBM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM ---\n",
      "balanced_accuracy   : 0.7772 ± 0.0155\n",
      "f1_macro            : 0.4502 ± 0.0019\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.7772 ± 0.0155\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=7,\n",
    "                num_leaves=50,\n",
    "                min_child_samples=30,\n",
    "                min_child_weight=1e-2,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=0.1,  # L1 정규화\n",
    "                reg_lambda=1.0,  # L2 정규화\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_lgbm = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores_lgbm = cross_validate(\n",
    "    lgbm_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_lgbm,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- LightGBM ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_lgbm[f\"test_{metric}\"].mean()\n",
    "    std = scores_lgbm[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient Boosting ---\n",
      "balanced_accuracy   : 0.7923 ± 0.0049\n",
      "f1_macro            : 0.4389 ± 0.0035\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.7923 ± 0.0049\n"
     ]
    }
   ],
   "source": [
    "### Gradient Boosting with Best Parameters\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
    "\n",
    "gb_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            GradientBoostingClassifier(\n",
    "                learning_rate=0.012287721406968568,\n",
    "                max_depth=2,\n",
    "                max_features=1.0,\n",
    "                min_samples_leaf=7,\n",
    "                n_estimators=340,\n",
    "                subsample=0.7257423924305306,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_gb = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores_gb = cross_validate(\n",
    "    gb_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_gb,\n",
    "    scoring=[\n",
    "        \"balanced_accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"precision_macro\",\n",
    "        \"recall_macro\",\n",
    "    ],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Gradient Boosting ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_gb[f\"test_{metric}\"].mean()\n",
    "    std = scores_gb[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논샘플링 버전 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosampling_models = {\n",
    "    \"RandomForest\": Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"clf\",\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=15,\n",
    "                    min_samples_split=10,\n",
    "                    min_samples_leaf=4,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"XGBoost\": Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"clf\",\n",
    "                XGBClassifier(\n",
    "                    n_estimators=500,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": Pipeline(\n",
    "        [\n",
    "            (\"clf\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# 모델별 최적 k 딕셔너리\n",
    "best_k_dict = {\n",
    "    \"LightGBM\": 10,\n",
    "    \"RandomForest\": 3,\n",
    "    \"XGBoost\": 10,\n",
    "}\n",
    "\n",
    "# 최적 k 기준으로 다시 교차 검증\n",
    "for model_name, k in best_k_dict.items():\n",
    "    print(f\"\\n--- {model_name} (k={k}) ---\")\n",
    "\n",
    "    pipe = nosampling_models[model_name]\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "        mean = scores[f\"test_{metric}\"].mean()\n",
    "        std = scores[f\"test_{metric}\"].std()\n",
    "        print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
