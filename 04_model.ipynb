{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 라이브러리 import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import BorderlineSMOTE, SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    balanced_accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_csv(\"final.csv\")\n",
    "\n",
    "# 정답 라벨 생성\n",
    "df[\"label\"] = (df[\"wildfire_count\"] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 추가 feature 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 시간 및 시즌 정보\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df[\"month\"] = df[\"datetime\"].dt.month\n",
    "df[\"weekday\"] = df[\"datetime\"].dt.weekday\n",
    "df[\"hour\"] = df[\"datetime\"].dt.hour\n",
    "df[\"is_weekend\"] = df[\"weekday\"].isin([5, 6]).astype(int)\n",
    "\n",
    "# 2. Lag·Rolling 추가\n",
    "df = df.sort_values([\"region\", \"datetime\"])\n",
    "df[\"temp_lag1\"] = df.groupby(\"region\")[\"temp\"].shift(1)\n",
    "df[\"temp_3day_mean\"] = df.groupby(\"region\")[\"temp\"].transform(\n",
    "    lambda x: x.rolling(window=72, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# 3. VPD (증기압 결핍량)\n",
    "df[\"VPD\"] = (\n",
    "    0.6108\n",
    "    * np.exp((17.27 * df[\"temp\"]) / (df[\"temp\"] + 237.3))\n",
    "    * (1 - df[\"humidity\"] / 100)\n",
    ")\n",
    "\n",
    "# 4. rain_presence 컬럼 생성\n",
    "df[\"rain_presence\"] = (df[\"rain_indicator\"] == 0).astype(int)\n",
    "\n",
    "# datetime 정렬 및 누적 계산\n",
    "df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "df = df.sort_values([\"region\", \"datetime\"])\n",
    "\n",
    "# 7일(=168시간) rolling 합계\n",
    "df[\"rain_presence_7day_sum\"] = df.groupby(\"region\")[\"rain_presence\"].transform(\n",
    "    lambda x: x.rolling(window=168, min_periods=1).sum()\n",
    ")\n",
    "\n",
    "df[\"label\"] = (df[\"wildfire_count\"] > 0).astype(int)\n",
    "df.fillna(0, inplace=True)  # Lag나 rolling 후 NaN 처리\n",
    "\n",
    "# 학습용 컬럼 선택\n",
    "feature_cols = [\n",
    "    \"temp\",\n",
    "    \"wind\",\n",
    "    \"humidity\",\n",
    "    \"VPD\",\n",
    "    \"rain_presence_7day_sum\",\n",
    "    \"month\",\n",
    "    \"weekday\",\n",
    "    \"hour\",\n",
    "    \"is_weekend\",\n",
    "    \"temp_lag1\",\n",
    "    \"temp_3day_mean\",\n",
    "]\n",
    "\n",
    "X = df[feature_cols]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# 샘플링\n",
    "under = RandomUnderSampler(sampling_strategy={0: 10000}, random_state=42)\n",
    "over = BorderlineSMOTE(sampling_strategy={1: 10000}, random_state=42, k_neighbors=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기본 모델별 파이프라인 - 하이퍼파라미터 튜닝 전"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"RandomForest\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\n",
    "                \"clf\",\n",
    "                RandomForestClassifier(\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"XGBoost\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\n",
    "                \"clf\",\n",
    "                XGBClassifier(\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\"clf\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "        ]\n",
    "    ),\n",
    "    \"GradientBoosting\": Pipeline(\n",
    "        [\n",
    "            (\"under\", under),\n",
    "            (\"over\", over),\n",
    "            (\"gb\", GradientBoostingClassifier(random_state=42)),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 튜닝 전 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM ---\n",
      "balanced_accuracy   : 0.7498 ± 0.0261\n",
      "f1_macro            : 0.4764 ± 0.0008\n",
      "precision_macro     : 0.5011 ± 0.0001\n",
      "recall_macro        : 0.7498 ± 0.0261\n",
      "\n",
      "--- RandomForest ---\n",
      "balanced_accuracy   : 0.7209 ± 0.0167\n",
      "f1_macro            : 0.4830 ± 0.0011\n",
      "precision_macro     : 0.5012 ± 0.0001\n",
      "recall_macro        : 0.7209 ± 0.0167\n",
      "\n",
      "--- XGBoost ---\n",
      "balanced_accuracy   : 0.7292 ± 0.0285\n",
      "f1_macro            : 0.4802 ± 0.0006\n",
      "precision_macro     : 0.5011 ± 0.0001\n",
      "recall_macro        : 0.7292 ± 0.0285\n",
      "\n",
      "--- GradientBoosting ---\n",
      "balanced_accuracy   : 0.7953 ± 0.0121\n",
      "f1_macro            : 0.4610 ± 0.0015\n",
      "precision_macro     : 0.5009 ± 0.0000\n",
      "recall_macro        : 0.7953 ± 0.0121\n"
     ]
    }
   ],
   "source": [
    "# 모델별 최적 k 딕셔너리\n",
    "best_k_dict = {\n",
    "    \"LightGBM\": 10,\n",
    "    \"RandomForest\": 5,\n",
    "    \"XGBoost\": 10,\n",
    "    \"GradientBoosting\": 5,\n",
    "}\n",
    "\n",
    "# 최적 k 기준으로 교차 검증\n",
    "for model_name, k in best_k_dict.items():\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "\n",
    "    pipe = models[model_name]\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "        mean = scores[f\"test_{metric}\"].mean()\n",
    "        std = scores[f\"test_{metric}\"].std()\n",
    "        print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter 튜닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RandomForest test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- RandomForest ---\n",
      "balanced_accuracy   : 0.8012 ± 0.0104\n",
      "f1_macro            : 0.4356 ± 0.0022\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.8012 ± 0.0104\n"
     ]
    }
   ],
   "source": [
    "rf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            RandomForestClassifier(\n",
    "                n_estimators=300,\n",
    "                max_depth=3,\n",
    "                min_samples_split=10,\n",
    "                min_samples_leaf=4,\n",
    "                max_features=\"sqrt\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_rf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores_rf = cross_validate(\n",
    "    rf_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_rf,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- RandomForest ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_rf[f\"test_{metric}\"].mean()\n",
    "    std = scores_rf[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. XGBoost test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- XGBoost ---\n",
      "balanced_accuracy   : 0.8020 ± 0.0162\n",
      "f1_macro            : 0.4379 ± 0.0020\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.8020 ± 0.0162\n"
     ]
    }
   ],
   "source": [
    "xgb_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            XGBClassifier(\n",
    "                n_estimators=300,\n",
    "                learning_rate=0.005,\n",
    "                max_depth=3,\n",
    "                subsample=0.6,\n",
    "                colsample_bytree=0.9,\n",
    "                eval_metric=\"auc\",\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_xgb = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores_xgb = cross_validate(\n",
    "    xgb_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_xgb,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- XGBoost ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_xgb[f\"test_{metric}\"].mean()\n",
    "    std = scores_xgb[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LightGBM test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- LightGBM ---\n",
      "balanced_accuracy   : 0.7818 ± 0.0219\n",
      "f1_macro            : 0.4674 ± 0.0007\n",
      "precision_macro     : 0.5010 ± 0.0001\n",
      "recall_macro        : 0.7818 ± 0.0219\n"
     ]
    }
   ],
   "source": [
    "lgbm_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            LGBMClassifier(\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.01,\n",
    "                max_depth=7,\n",
    "                num_leaves=50,\n",
    "                min_child_samples=30,\n",
    "                min_child_weight=1e-2,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.7,\n",
    "                reg_alpha=0.1,  # L1 정규화\n",
    "                reg_lambda=1.0,  # L2 정규화\n",
    "                random_state=42,\n",
    "                verbose=-1,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_lgbm = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores_lgbm = cross_validate(\n",
    "    lgbm_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_lgbm,\n",
    "    scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- LightGBM ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_lgbm[f\"test_{metric}\"].mean()\n",
    "    std = scores_lgbm[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Gradient Boosting test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Gradient Boosting ---\n",
      "balanced_accuracy   : 0.8055 ± 0.0075\n",
      "f1_macro            : 0.4431 ± 0.0020\n",
      "precision_macro     : 0.5007 ± 0.0000\n",
      "recall_macro        : 0.8055 ± 0.0075\n"
     ]
    }
   ],
   "source": [
    "gb_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"under\", under),\n",
    "        (\"over\", over),\n",
    "        (\n",
    "            \"clf\",\n",
    "            GradientBoostingClassifier(\n",
    "                learning_rate=0.012287721406968568,\n",
    "                max_depth=2,\n",
    "                max_features=1.0,\n",
    "                min_samples_leaf=7,\n",
    "                n_estimators=340,\n",
    "                subsample=0.7257423924305306,\n",
    "                random_state=42,\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "cv_gb = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "scores_gb = cross_validate(\n",
    "    gb_pipeline,\n",
    "    X,\n",
    "    y,\n",
    "    cv=cv_gb,\n",
    "    scoring=[\n",
    "        \"balanced_accuracy\",\n",
    "        \"f1_macro\",\n",
    "        \"precision_macro\",\n",
    "        \"recall_macro\",\n",
    "    ],\n",
    "    return_train_score=False,\n",
    ")\n",
    "\n",
    "print(\"\\n--- Gradient Boosting ---\")\n",
    "for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "    mean = scores_gb[f\"test_{metric}\"].mean()\n",
    "    std = scores_gb[f\"test_{metric}\"].std()\n",
    "    print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Ensemble test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Ensemble ---\n",
      "balanced_accuracy   : 0.8076 ± 0.0091\n",
      "f1_macro            : 0.4488 ± 0.0011\n",
      "precision_macro     : 0.5008 ± 0.0000\n",
      "recall_macro        : 0.8076 ± 0.0091\n"
     ]
    }
   ],
   "source": [
    "# Ensemble 용 샘플링 정의\n",
    "under = RandomUnderSampler(sampling_strategy={0: 7000}, random_state=42)\n",
    "over = SMOTE(sampling_strategy={1: 10000}, random_state=42)\n",
    "\n",
    "# 개별 분류기 정의\n",
    "gb_clf = GradientBoostingClassifier(\n",
    "    n_estimators=340,\n",
    "    learning_rate=0.0123,\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=7,\n",
    "    max_features=1.0,\n",
    "    subsample=0.726,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.02,\n",
    "    max_depth=3,\n",
    "    subsample=0.8,\n",
    "    eval_metric=\"logloss\",\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=200, max_depth=10, random_state=42)\n",
    "\n",
    "# VotingClassifier 구성\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        (\"gb\", gb_clf),\n",
    "        (\"xgb\", xgb_clf),\n",
    "        (\"rf\", rf_clf),\n",
    "    ],\n",
    "    voting=\"soft\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "\n",
    "# 교차 검증\n",
    "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "ba_list, pm_list, rm_list, fm_list = [], [], [], []\n",
    "\n",
    "for fold_idx, (train_idx, test_idx) in enumerate(cv.split(X, y), start=1):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # 샘플링 수행\n",
    "    X_resampled, y_resampled = under.fit_resample(X_train, y_train)\n",
    "    X_resampled, y_resampled = over.fit_resample(X_resampled, y_resampled)\n",
    "\n",
    "    # 학습 및 예측\n",
    "    voting_clf.fit(X_resampled, y_resampled)\n",
    "    y_pred = voting_clf.predict(X_test)\n",
    "\n",
    "    # 평가\n",
    "    ba = balanced_accuracy_score(y_test, y_pred)\n",
    "    pm = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    rm = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "    fm = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "\n",
    "    ba_list.append(ba)\n",
    "    pm_list.append(pm)\n",
    "    rm_list.append(rm)\n",
    "    fm_list.append(fm)\n",
    "\n",
    "# 최종 요약\n",
    "print(\"\\n--- Ensemble ---\")\n",
    "print(f\"balanced_accuracy   : {np.mean(ba_list):.4f} ± {np.std(ba_list):.4f}\")\n",
    "print(f\"f1_macro            : {np.mean(fm_list):.4f} ± {np.std(fm_list):.4f}\")\n",
    "print(f\"precision_macro     : {np.mean(pm_list):.4f} ± {np.std(pm_list):.4f}\")\n",
    "print(f\"recall_macro        : {np.mean(rm_list):.4f} ± {np.std(rm_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 논샘플링 버전 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "nosampling_models = {\n",
    "    \"RandomForest\": Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"clf\",\n",
    "                RandomForestClassifier(\n",
    "                    n_estimators=300,\n",
    "                    max_depth=15,\n",
    "                    min_samples_split=10,\n",
    "                    min_samples_leaf=4,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"XGBoost\": Pipeline(\n",
    "        [\n",
    "            (\n",
    "                \"clf\",\n",
    "                XGBClassifier(\n",
    "                    n_estimators=500,\n",
    "                    learning_rate=0.05,\n",
    "                    max_depth=6,\n",
    "                    subsample=0.8,\n",
    "                    colsample_bytree=0.8,\n",
    "                    use_label_encoder=False,\n",
    "                    eval_metric=\"logloss\",\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "        ]\n",
    "    ),\n",
    "    \"LightGBM\": Pipeline(\n",
    "        [\n",
    "            (\"clf\", LGBMClassifier(random_state=42, verbose=-1)),\n",
    "        ]\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_validate\n",
    "\n",
    "# 모델별 최적 k 딕셔너리\n",
    "best_k_dict = {\n",
    "    \"LightGBM\": 10,\n",
    "    \"RandomForest\": 3,\n",
    "    \"XGBoost\": 10,\n",
    "}\n",
    "\n",
    "# 최적 k 기준으로 다시 교차 검증\n",
    "for model_name, k in best_k_dict.items():\n",
    "    print(f\"\\n--- {model_name} (k={k}) ---\")\n",
    "\n",
    "    pipe = nosampling_models[model_name]\n",
    "    cv = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "    scores = cross_validate(\n",
    "        pipe,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        scoring=[\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"],\n",
    "        return_train_score=False,\n",
    "    )\n",
    "\n",
    "    for metric in [\"balanced_accuracy\", \"f1_macro\", \"precision_macro\", \"recall_macro\"]:\n",
    "        mean = scores[f\"test_{metric}\"].mean()\n",
    "        std = scores[f\"test_{metric}\"].std()\n",
    "        print(f\"{metric:<20}: {mean:.4f} ± {std:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
