{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wildfire ë°ì´í„° ì§€ì—­ëª… ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ê¸°ì¤€ ì •ì˜\n",
    "metropolitan_cities = [\n",
    "    \"ì„œìš¸\",\n",
    "    \"ë¶€ì‚°\",\n",
    "    \"ëŒ€êµ¬\",\n",
    "    \"ì¸ì²œ\",\n",
    "    \"ê´‘ì£¼\",\n",
    "    \"ëŒ€ì „\",\n",
    "    \"ìš¸ì‚°\",\n",
    "    \"ì„¸ì¢…\",\n",
    "    \"ì œì£¼\",\n",
    "]\n",
    "exclude_cut = [\"ë‚¨ì–‘ì£¼\", \"ë™ë‘ì²œ\"]\n",
    "\n",
    "for year in range(2020, 2025):\n",
    "    try:\n",
    "        # ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "        path = f\"wildFire/wildfire_data/wildfire_{year}.csv\"\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # ì§€ì—­ ì²˜ë¦¬ í•¨ìˆ˜\n",
    "        def resolve_region(row):\n",
    "            gungu = str(row.get(\"locgungu\", \"\")).strip()\n",
    "\n",
    "            if gungu in metropolitan_cities:\n",
    "                return gungu\n",
    "            elif gungu in exclude_cut:\n",
    "                return gungu\n",
    "            elif gungu and gungu.lower() != \"nan\":\n",
    "                # ê³µë°±ì´ ìˆëŠ” ê²½ìš° ì²« ë²ˆì§¸ ë‹¨ì–´ë§Œ ì‚¬ìš©\n",
    "                if \" \" in gungu:\n",
    "                    return gungu.split()[0]\n",
    "                # 3ê¸€ì ì´ìƒì¸ ê²½ìš° ì• ë‘ ê¸€ìë§Œ ì‚¬ìš©\n",
    "                return gungu[:2] if len(gungu) >= 3 else gungu\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        df[\"region\"] = df.apply(resolve_region, axis=1)\n",
    "\n",
    "        # ì €ì¥\n",
    "        save_path = f\"wildFire/wildfire_{year}_processed.csv\"\n",
    "        df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"âœ… {year}ë…„ ì²˜ë¦¬ ì™„ë£Œ â†’ {save_path}\")\n",
    "        print(f\"  ê³ ìœ  ì§€ì—­ ìˆ˜: {df['region'].nunique()}ê°œ\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {year}ë…„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weather ë°ì´í„° ì§€ì—­ëª… ì „ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# ì „ì²˜ë¦¬ ê·œì¹™ ì •ì˜\n",
    "keep_names = [\"ëŒ€ê´€ë ¹\", \"ë™ë‘ì²œ\", \"ë°±ë ¹ë„\", \"ì„œê·€í¬\", \"ìš¸ë¦‰ë„\", \"ì¶”í’ë ¹\", \"í‘ì‚°ë„\"]\n",
    "prefix_remove = [\"ë¶ê°•ë¦‰\", \"ë¶ì°½ì›\", \"ë¶ì¶˜ì²œ\", \"ë¶ë¶€ì‚°\", \"ì„œì²­ì£¼\"]\n",
    "\n",
    "\n",
    "# ì§€ì ëª… ì •ì œ í•¨ìˆ˜\n",
    "def clean_station_name(name):\n",
    "    if isinstance(name, str) and len(name) == 3:\n",
    "        if name in keep_names:\n",
    "            return name\n",
    "        elif name in prefix_remove:\n",
    "            return name[1:]\n",
    "        elif name.endswith((\"êµ°\", \"ì‹œ\")):\n",
    "            return name[:-1]\n",
    "    return name\n",
    "\n",
    "\n",
    "# ë°ì´í„° ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "data_dir = \"/Users/shinyeonseong/source/repos/2-1/MachineLearning1/TermProject/wildFire/weather_data\"\n",
    "\n",
    "# 2020ë…„ë¶€í„° 2024ë…„ê¹Œì§€ì˜ ë°ì´í„° ì²˜ë¦¬\n",
    "for year in range(2020, 2025):\n",
    "    input_file = os.path.join(data_dir, f\"weather_{year}.csv\")\n",
    "    output_file = os.path.join(data_dir, f\"weather_{year}_processed.csv\")\n",
    "\n",
    "    try:\n",
    "        # CSV íŒŒì¼ ì½ê¸°\n",
    "        print(f\"\\n{year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘...\")\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # ì§€ì ëª… ì „ì²˜ë¦¬ ì ìš©\n",
    "        df[\"ì§€ì ëª…\"] = df[\"ì§€ì ëª…\"].apply(clean_station_name)\n",
    "\n",
    "        # ì „ì²˜ë¦¬ëœ íŒŒì¼ ì €ì¥\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"{year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ê²½ê³ : {year}ë…„ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\")\n",
    "    except Exception as e:\n",
    "        print(f\"ì˜¤ë¥˜: {year}ë…„ ë°ì´í„° ì²˜ë¦¬ ì¤‘ ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {str(e)}\")\n",
    "\n",
    "print(\"\\nëª¨ë“  ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‚°ë¶ˆ ë°œìƒ íšŸìˆ˜ wildfire_count ì»¬ëŸ¼ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for year in range(2020, 2025):\n",
    "    print(f\"=== {year}ë…„ ì²˜ë¦¬ ì‹œì‘ ===\")\n",
    "    weather_path = f\"wildFire/weather_processed/weather_{year}_processed.csv\"\n",
    "    wildfire_path = f\"wildFire/wildfire_processed/wildfire_{year}_processed.csv\"\n",
    "    try:\n",
    "        # -------------------------\n",
    "        # 1. weather ë°ì´í„° ì²˜ë¦¬\n",
    "        # -------------------------\n",
    "        weather_df = pd.read_csv(weather_path, encoding=\"utf-8-sig\")\n",
    "        weather_df[\"ì¼ì‹œ\"] = pd.to_datetime(weather_df[\"ì¼ì‹œ\"])\n",
    "        # ë‚ ì§œëŠ” ìœ ì§€í•˜ê³  ì‹œê°„ë§Œ HH:MM í˜•ì‹ìœ¼ë¡œ ë³€ê²½\n",
    "        weather_df[\"hour\"] = weather_df[\"ì¼ì‹œ\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        weather_df = weather_df.rename(columns={\"ì§€ì ëª…\": \"region\"})\n",
    "\n",
    "        for col in [\"ê¸°ì˜¨(Â°C)\", \"ê°•ìˆ˜ëŸ‰(mm)\", \"í’ì†(m/s)\", \"ìŠµë„(%)\"]:\n",
    "            weather_df[col] = pd.to_numeric(weather_df[col], errors=\"coerce\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. wildfire ë°ì´í„° ì²˜ë¦¬\n",
    "        # -------------------------\n",
    "        wildfire_df = pd.read_csv(wildfire_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # ì‹œê°„ íŒŒì‹± ì•ˆì „í•˜ê²Œ ì²˜ë¦¬\n",
    "        wildfire_df[\"datetime\"] = pd.to_datetime(\n",
    "            wildfire_df[\"startyear\"].astype(str).str.zfill(4)\n",
    "            + \"-\"\n",
    "            + wildfire_df[\"startmonth\"].astype(str).str.zfill(2)\n",
    "            + \"-\"\n",
    "            + wildfire_df[\"startday\"].astype(str).str.zfill(2)\n",
    "            + \" \"\n",
    "            + wildfire_df[\"starttime\"].fillna(\"00:00:00\").str.slice(0, 8),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        # ë‚ ì§œëŠ” ìœ ì§€í•˜ê³  ì‹œê°„ë§Œ HH:MM í˜•ì‹ìœ¼ë¡œ ë³€ê²½\n",
    "        wildfire_df[\"hour\"] = (\n",
    "            wildfire_df[\"datetime\"].dt.floor(\"h\").dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        )\n",
    "\n",
    "        # ì§€ì—­ ì²˜ë¦¬: ê´‘ì—­ì‹œ/íŠ¹ë³„ì‹œ/íŠ¹ë³„ìì¹˜ì‹œ/ë„ êµ¬ë¶„\n",
    "        metropolitan_cities = [\n",
    "            \"ê´‘ì£¼\",\n",
    "            \"ëŒ€êµ¬\",\n",
    "            \"ë¶€ì‚°\",\n",
    "            \"ì„œìš¸\",\n",
    "            \"ì„¸ì¢…\",\n",
    "            \"ìš¸ì‚°\",\n",
    "            \"ì¸ì²œ\",\n",
    "        ]\n",
    "        wildfire_df[\"region\"] = wildfire_df.apply(\n",
    "            lambda row: (\n",
    "                row[\"locsi\"] if row[\"locsi\"] in metropolitan_cities else row[\"locgungu\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. ë°œìƒ ì—¬ë¶€ ë§¤í•‘\n",
    "        # -------------------------\n",
    "\n",
    "        # ì‚°ë¶ˆ ë°œìƒ ì‹œê°+ì§€ì—­ë³„ ì¹´ìš´íŠ¸\n",
    "        wildfire_counts = (\n",
    "            wildfire_df.groupby([\"region\", \"hour\"]).size().reset_index(name=\"count\")\n",
    "        )\n",
    "\n",
    "        # ë§¤í•‘ ë¡œì§ ê°œì„ \n",
    "        def count_wildfires(row):\n",
    "            # ê°™ì€ ì§€ì—­ì˜ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "            region_matches = wildfire_counts[wildfire_counts[\"region\"] == row[\"region\"]]\n",
    "            if len(region_matches) == 0:\n",
    "                return 0\n",
    "\n",
    "            # ê°™ì€ ì‹œê°„ì˜ ë°ì´í„°ë§Œ í•„í„°ë§\n",
    "            time_matches = region_matches[region_matches[\"hour\"] == row[\"hour\"]]\n",
    "            return time_matches[\"count\"].sum() if len(time_matches) > 0 else 0\n",
    "\n",
    "        weather_df[\"wildfire_count\"] = weather_df.apply(count_wildfires, axis=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥\n",
    "        # -------------------------\n",
    "        final = weather_df[\n",
    "            [\n",
    "                \"hour\",\n",
    "                \"region\",\n",
    "                \"ê¸°ì˜¨(Â°C)\",\n",
    "                \"ê°•ìˆ˜ëŸ‰(mm)\",\n",
    "                \"í’ì†(m/s)\",\n",
    "                \"ìŠµë„(%)\",\n",
    "                \"wildfire_count\",\n",
    "            ]\n",
    "        ].rename(\n",
    "            columns={\n",
    "                \"hour\": \"datetime\",\n",
    "                \"ê¸°ì˜¨(Â°C)\": \"temp\",\n",
    "                \"ê°•ìˆ˜ëŸ‰(mm)\": \"rain\",\n",
    "                \"í’ì†(m/s)\": \"wind\",\n",
    "                \"ìŠµë„(%)\": \"humidity\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for col in [\"temp\", \"rain\", \"wind\", \"humidity\"]:\n",
    "            final[col] = final[col].round(3)\n",
    "\n",
    "        output_path = f\"wildFire/final_{year}.csv\"\n",
    "        final.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"âœ… {year}ë…„ ìµœì¢… ë°ì´í„° ìƒì„± ì™„ë£Œ! ìƒ˜í”Œ:\")\n",
    "        print(final.head())\n",
    "        print(\"ğŸ“Š wildfire_count ë¶„í¬:\")\n",
    "        print(final[\"wildfire_count\"].value_counts())\n",
    "        print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ {year}ë…„ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indicator column ì¶”ê°€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data/final_2020.csv ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\n",
      "final_data/final_2021.csv ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\n",
      "final_data/final_2022.csv ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\n",
      "final_data/final_2023.csv ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\n",
      "final_data/final_2024.csv ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ì²˜ë¦¬í•  íŒŒì¼ ë¦¬ìŠ¤íŠ¸\n",
    "file_list = [\n",
    "    \"final_data/final_2020.csv\",\n",
    "    \"final_data/final_2021.csv\",\n",
    "    \"final_data/final_2022.csv\",\n",
    "    \"final_data/final_2023.csv\",\n",
    "    \"final_data/final_2024.csv\",\n",
    "]\n",
    "\n",
    "for file_path in file_list:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # rain ê°’ì´ ê²°ì¸¡ì´ë©´ 1, ì•„ë‹ˆë©´ rain ê°’ ê·¸ëŒ€ë¡œ ë³µì‚¬\n",
    "    df[\"rain_indicator\"] = df[\"rain\"].where(df[\"rain\"].notnull(), 1)\n",
    "\n",
    "    # ë®ì–´ì“°ê¸°\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"{file_path} ì²˜ë¦¬ ì™„ë£Œ (rain_indicator ì»¬ëŸ¼ ì¶”ê°€)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì§€ì—­ë³„ ë°ì´í„°ì…‹ ë¶„ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ 91ê°œì˜ ì§€ì—­ë³„ CSV íŒŒì¼ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸°\n",
    "df = pd.read_csv(\"final_2020-2024.csv\")  # íŒŒì¼ëª…ì— ë§ê²Œ ìˆ˜ì •\n",
    "\n",
    "# region ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë‚˜ëˆ”\n",
    "region_groups = {region: group_df for region, group_df in df.groupby(\"region\")}\n",
    "\n",
    "# ê° ê·¸ë£¹ì„ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "for region, group_df in region_groups.items():\n",
    "    # ì§€ì—­ëª…ì„ íŒŒì¼ëª…ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ íŒŒì¼ëª… ì•ˆì „í•˜ê²Œ ì²˜ë¦¬ (ì˜ˆ: ê³µë°± ì œê±°, íŠ¹ìˆ˜ë¬¸ì ì œê±° ë“± í•„ìš” ì‹œ ì¶”ê°€)\n",
    "    safe_region = region.replace(\" \", \"_\")\n",
    "    filename = f\"{safe_region}_data.csv\"\n",
    "    group_df.to_csv(filename, index=False)\n",
    "\n",
    "# ì „ì²´ ëª‡ ê°œì˜ CSV íŒŒì¼ë¡œ ë‚˜ë‰˜ì—ˆëŠ”ì§€ ì¶œë ¥\n",
    "print(f\"ì´ {len(region_groups)}ê°œì˜ ì§€ì—­ë³„ CSV íŒŒì¼ë¡œ ë‚˜ëˆ„ì–´ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN(temp, humidity, wind ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬) - (1)\n",
    "#### Elbow method ì‚¬ìš©"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ìˆ˜ì¹˜í˜•ë§Œ\n",
    "numeric_df = sample_df[numeric_cols].copy()\n",
    "original_nan_mask = numeric_df.isna()\n",
    "mask_candidates = ~original_nan_mask\n",
    "random_mask = np.random.rand(*numeric_df.shape) < 0.1\n",
    "mask = random_mask & mask_candidates\n",
    "\n",
    "# ì¸ìœ„ì  ê²°ì¸¡ ìƒì„±\n",
    "masked_df = numeric_df.copy()\n",
    "masked_df[mask] = np.nan\n",
    "\n",
    "# Elbow loop\n",
    "rmse_list = []\n",
    "for k in range(1, 16):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    imputed = imputer.fit_transform(masked_df)\n",
    "\n",
    "    # 1. flattenëœ mask ìœ„ì¹˜ì—ì„œì˜ ê°’ ì¶”ì¶œ\n",
    "    true_vals = numeric_df.values[mask]  # ì›ë˜ ê°’\n",
    "    pred_vals = imputed[mask]  # ë³µì›ëœ ê°’\n",
    "\n",
    "    # 2. ê¸¸ì´ í™•ì¸ (ë””ë²„ê¹…ìš©)\n",
    "    print(f\"k={k} | true={true_vals.shape}, pred={pred_vals.shape}\")\n",
    "\n",
    "    # 3. RMSE ê³„ì‚° ë° ì €ì¥\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, pred_vals))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1, 16), rmse_list, marker=\"o\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Elbow Method for Optimal n_neighbors\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN(temp, humidity, wind ì»¬ëŸ¼ ê²°ì¸¡ì¹˜ ì²˜ë¦¬) - (2)\n",
    "#### KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ë°©ë²• 1. KNN Imputation\n",
    "### KNN Imputation: ê²°ì¸¡ì¹˜ë¥¼ ê°€ì¥ ìœ ì‚¬í•œ Kê°œì˜ ì´ì›ƒê°’ì„ í‰ê· /ê°€ì¤‘ í‰ê· í•˜ì—¬ ì±„ìš°ëŠ” ë°©ì‹\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# KNN ì „ìš© ë°ì´í„°í”„ë ˆì„ df_KNN ìƒì„±\n",
    "df_KNN = df_data_all\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ë³€ìˆ˜ë§Œ ëŒ€ìƒìœ¼ë¡œ KNN Imputation ìˆ˜í–‰\n",
    "# KNNImputerëŠ” ìˆ˜ì¹˜í˜•ë§Œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë¯€ë¡œ ë¬¸ìì—´/ë²”ì£¼í˜• ì œì™¸\n",
    "num_cols = df_KNN.select_dtypes(include=[np.number]).columns\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# ê²°ê³¼ ì €ì¥ìš© ë¦¬ìŠ¤íŠ¸\n",
    "imputed_groups = []\n",
    "\n",
    "# ìˆ˜ì¹˜í˜• ì»¬ëŸ¼\n",
    "num_cols = df_KNN.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# ì§€ì—­ë³„ë¡œ ë‚˜ëˆ„ì–´ KNN Imputer ì ìš©\n",
    "imputed_groups = []\n",
    "for region_name, group in df_KNN.groupby(\"region\"):\n",
    "    group_copy = group.copy()\n",
    "\n",
    "    # í•´ë‹¹ ê·¸ë£¹ ë‚´ ìˆ˜ì¹˜í˜• ì»¬ëŸ¼ì— ëŒ€í•´ì„œë§Œ KNN ì ìš©\n",
    "    # optimal k = 4 (elbow method ì°¸ì¡°)\n",
    "    imputer = KNNImputer(n_neighbors=4)\n",
    "    group_copy[num_cols] = imputer.fit_transform(group_copy[num_cols])\n",
    "\n",
    "    imputed_groups.append(group_copy)\n",
    "\n",
    "# ë³‘í•©\n",
    "df_KNN_imputed = pd.concat(imputed_groups).sort_index()\n",
    "\n",
    "# ê²°ê³¼ í™•ì¸\n",
    "print(\"âœ… ì§€ì—­ë³„ KNN Imputation ì™„ë£Œ!\")\n",
    "print(\"ğŸ“¦ ìµœì¢… ë°ì´í„° í¬ê¸°:\", df_KNN_imputed.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
