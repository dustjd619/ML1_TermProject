{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### wildfire 데이터 지역명 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 기준 정의\n",
    "metropolitan_cities = [\n",
    "    \"서울\",\n",
    "    \"부산\",\n",
    "    \"대구\",\n",
    "    \"인천\",\n",
    "    \"광주\",\n",
    "    \"대전\",\n",
    "    \"울산\",\n",
    "    \"세종\",\n",
    "    \"제주\",\n",
    "]\n",
    "exclude_cut = [\"남양주\", \"동두천\"]\n",
    "\n",
    "for year in range(2020, 2025):\n",
    "    try:\n",
    "        # 데이터 불러오기\n",
    "        path = f\"wildFire/wildfire_data/wildfire_{year}.csv\"\n",
    "        df = pd.read_csv(path, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # 지역 처리 함수\n",
    "        def resolve_region(row):\n",
    "            gungu = str(row.get(\"locgungu\", \"\")).strip()\n",
    "\n",
    "            if gungu in metropolitan_cities:\n",
    "                return gungu\n",
    "            elif gungu in exclude_cut:\n",
    "                return gungu\n",
    "            elif gungu and gungu.lower() != \"nan\":\n",
    "                # 공백이 있는 경우 첫 번째 단어만 사용\n",
    "                if \" \" in gungu:\n",
    "                    return gungu.split()[0]\n",
    "                # 3글자 이상인 경우 앞 두 글자만 사용\n",
    "                return gungu[:2] if len(gungu) >= 3 else gungu\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "        df[\"region\"] = df.apply(resolve_region, axis=1)\n",
    "\n",
    "        # 저장\n",
    "        save_path = f\"wildFire/wildfire_{year}_processed.csv\"\n",
    "        df.to_csv(save_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"✅ {year}년 처리 완료 → {save_path}\")\n",
    "        print(f\"  고유 지역 수: {df['region'].nunique()}개\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {year}년 처리 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### weather 데이터 지역명 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 전처리 규칙 정의\n",
    "keep_names = [\"대관령\", \"동두천\", \"백령도\", \"서귀포\", \"울릉도\", \"추풍령\", \"흑산도\"]\n",
    "prefix_remove = [\"북강릉\", \"북창원\", \"북춘천\", \"북부산\", \"서청주\"]\n",
    "\n",
    "\n",
    "# 지점명 정제 함수\n",
    "def clean_station_name(name):\n",
    "    if isinstance(name, str) and len(name) == 3:\n",
    "        if name in keep_names:\n",
    "            return name\n",
    "        elif name in prefix_remove:\n",
    "            return name[1:]\n",
    "        elif name.endswith((\"군\", \"시\")):\n",
    "            return name[:-1]\n",
    "    return name\n",
    "\n",
    "\n",
    "# 데이터 디렉토리 경로\n",
    "data_dir = \"/Users/shinyeonseong/source/repos/2-1/MachineLearning1/TermProject/wildFire/weather_data\"\n",
    "\n",
    "# 2020년부터 2024년까지의 데이터 처리\n",
    "for year in range(2020, 2025):\n",
    "    input_file = os.path.join(data_dir, f\"weather_{year}.csv\")\n",
    "    output_file = os.path.join(data_dir, f\"weather_{year}_processed.csv\")\n",
    "\n",
    "    try:\n",
    "        # CSV 파일 읽기\n",
    "        print(f\"\\n{year}년 데이터 처리 중...\")\n",
    "        df = pd.read_csv(input_file)\n",
    "\n",
    "        # 지점명 전처리 적용\n",
    "        df[\"지점명\"] = df[\"지점명\"].apply(clean_station_name)\n",
    "\n",
    "        # 전처리된 파일 저장\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"{year}년 데이터 처리 완료: {output_file}\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"경고: {year}년 데이터 파일을 찾을 수 없습니다.\")\n",
    "    except Exception as e:\n",
    "        print(f\"오류: {year}년 데이터 처리 중 문제가 발생했습니다: {str(e)}\")\n",
    "\n",
    "print(\"\\n모든 데이터 처리 완료!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 산불 발생 횟수 wildfire_count 컬럼 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for year in range(2020, 2025):\n",
    "    print(f\"=== {year}년 처리 시작 ===\")\n",
    "    weather_path = f\"wildFire/weather_processed/weather_{year}_processed.csv\"\n",
    "    wildfire_path = f\"wildFire/wildfire_processed/wildfire_{year}_processed.csv\"\n",
    "    try:\n",
    "        # -------------------------\n",
    "        # 1. weather 데이터 처리\n",
    "        # -------------------------\n",
    "        weather_df = pd.read_csv(weather_path, encoding=\"utf-8-sig\")\n",
    "        weather_df[\"일시\"] = pd.to_datetime(weather_df[\"일시\"])\n",
    "        # 날짜는 유지하고 시간만 HH:MM 형식으로 변경\n",
    "        weather_df[\"hour\"] = weather_df[\"일시\"].dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        weather_df = weather_df.rename(columns={\"지점명\": \"region\"})\n",
    "\n",
    "        for col in [\"기온(°C)\", \"강수량(mm)\", \"풍속(m/s)\", \"습도(%)\"]:\n",
    "            weather_df[col] = pd.to_numeric(weather_df[col], errors=\"coerce\")\n",
    "\n",
    "        # -------------------------\n",
    "        # 2. wildfire 데이터 처리\n",
    "        # -------------------------\n",
    "        wildfire_df = pd.read_csv(wildfire_path, encoding=\"utf-8-sig\")\n",
    "\n",
    "        # 시간 파싱 안전하게 처리\n",
    "        wildfire_df[\"datetime\"] = pd.to_datetime(\n",
    "            wildfire_df[\"startyear\"].astype(str).str.zfill(4)\n",
    "            + \"-\"\n",
    "            + wildfire_df[\"startmonth\"].astype(str).str.zfill(2)\n",
    "            + \"-\"\n",
    "            + wildfire_df[\"startday\"].astype(str).str.zfill(2)\n",
    "            + \" \"\n",
    "            + wildfire_df[\"starttime\"].fillna(\"00:00:00\").str.slice(0, 8),\n",
    "            errors=\"coerce\",\n",
    "        )\n",
    "        # 날짜는 유지하고 시간만 HH:MM 형식으로 변경\n",
    "        wildfire_df[\"hour\"] = (\n",
    "            wildfire_df[\"datetime\"].dt.floor(\"h\").dt.strftime(\"%Y-%m-%d %H:%M\")\n",
    "        )\n",
    "\n",
    "        # 지역 처리: 광역시/특별시/특별자치시/도 구분\n",
    "        metropolitan_cities = [\n",
    "            \"광주\",\n",
    "            \"대구\",\n",
    "            \"부산\",\n",
    "            \"서울\",\n",
    "            \"세종\",\n",
    "            \"울산\",\n",
    "            \"인천\",\n",
    "        ]\n",
    "        wildfire_df[\"region\"] = wildfire_df.apply(\n",
    "            lambda row: (\n",
    "                row[\"locsi\"] if row[\"locsi\"] in metropolitan_cities else row[\"locgungu\"]\n",
    "            ),\n",
    "            axis=1,\n",
    "        )\n",
    "\n",
    "        # -------------------------\n",
    "        # 3. 발생 여부 매핑\n",
    "        # -------------------------\n",
    "\n",
    "        # 산불 발생 시각+지역별 카운트\n",
    "        wildfire_counts = (\n",
    "            wildfire_df.groupby([\"region\", \"hour\"]).size().reset_index(name=\"count\")\n",
    "        )\n",
    "\n",
    "        # 매핑 로직 개선\n",
    "        def count_wildfires(row):\n",
    "            # 같은 지역의 데이터만 필터링\n",
    "            region_matches = wildfire_counts[wildfire_counts[\"region\"] == row[\"region\"]]\n",
    "            if len(region_matches) == 0:\n",
    "                return 0\n",
    "\n",
    "            # 같은 시간의 데이터만 필터링\n",
    "            time_matches = region_matches[region_matches[\"hour\"] == row[\"hour\"]]\n",
    "            return time_matches[\"count\"].sum() if len(time_matches) > 0 else 0\n",
    "\n",
    "        weather_df[\"wildfire_count\"] = weather_df.apply(count_wildfires, axis=1)\n",
    "\n",
    "        # -------------------------\n",
    "        # 4. 결과 정리 및 저장\n",
    "        # -------------------------\n",
    "        final = weather_df[\n",
    "            [\n",
    "                \"hour\",\n",
    "                \"region\",\n",
    "                \"기온(°C)\",\n",
    "                \"강수량(mm)\",\n",
    "                \"풍속(m/s)\",\n",
    "                \"습도(%)\",\n",
    "                \"wildfire_count\",\n",
    "            ]\n",
    "        ].rename(\n",
    "            columns={\n",
    "                \"hour\": \"datetime\",\n",
    "                \"기온(°C)\": \"temp\",\n",
    "                \"강수량(mm)\": \"rain\",\n",
    "                \"풍속(m/s)\": \"wind\",\n",
    "                \"습도(%)\": \"humidity\",\n",
    "            }\n",
    "        )\n",
    "\n",
    "        for col in [\"temp\", \"rain\", \"wind\", \"humidity\"]:\n",
    "            final[col] = final[col].round(3)\n",
    "\n",
    "        output_path = f\"wildFire/final_{year}.csv\"\n",
    "        final.to_csv(output_path, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "        print(f\"✅ {year}년 최종 데이터 생성 완료! 샘플:\")\n",
    "        print(final.head())\n",
    "        print(\"📊 wildfire_count 분포:\")\n",
    "        print(final[\"wildfire_count\"].value_counts())\n",
    "        print()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ {year}년 처리 중 오류 발생: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indicator column 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_data/final_2020.csv 처리 완료 (rain_indicator 컬럼 추가)\n",
      "final_data/final_2021.csv 처리 완료 (rain_indicator 컬럼 추가)\n",
      "final_data/final_2022.csv 처리 완료 (rain_indicator 컬럼 추가)\n",
      "final_data/final_2023.csv 처리 완료 (rain_indicator 컬럼 추가)\n",
      "final_data/final_2024.csv 처리 완료 (rain_indicator 컬럼 추가)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 처리할 파일 리스트\n",
    "file_list = [\n",
    "    \"final_data/final_2020.csv\",\n",
    "    \"final_data/final_2021.csv\",\n",
    "    \"final_data/final_2022.csv\",\n",
    "    \"final_data/final_2023.csv\",\n",
    "    \"final_data/final_2024.csv\",\n",
    "]\n",
    "\n",
    "for file_path in file_list:\n",
    "    df = pd.read_csv(file_path)\n",
    "\n",
    "    # rain 값이 결측이면 1, 아니면 rain 값 그대로 복사\n",
    "    df[\"rain_indicator\"] = df[\"rain\"].where(df[\"rain\"].notnull(), 1)\n",
    "\n",
    "    # 덮어쓰기\n",
    "    df.to_csv(file_path, index=False)\n",
    "    print(f\"{file_path} 처리 완료 (rain_indicator 컬럼 추가)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 지역별 데이터셋 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 91개의 지역별 CSV 파일로 나누어 저장되었습니다.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# CSV 파일 불러오기\n",
    "df = pd.read_csv(\"final_2020-2024.csv\")  # 파일명에 맞게 수정\n",
    "\n",
    "# region 별로 그룹화하여 나눔\n",
    "region_groups = {region: group_df for region, group_df in df.groupby(\"region\")}\n",
    "\n",
    "# 각 그룹을 CSV 파일로 저장\n",
    "for region, group_df in region_groups.items():\n",
    "    # 지역명을 파일명으로 사용할 수 있도록 파일명 안전하게 처리 (예: 공백 제거, 특수문자 제거 등 필요 시 추가)\n",
    "    safe_region = region.replace(\" \", \"_\")\n",
    "    filename = f\"{safe_region}_data.csv\"\n",
    "    group_df.to_csv(filename, index=False)\n",
    "\n",
    "# 전체 몇 개의 CSV 파일로 나뉘었는지 출력\n",
    "print(f\"총 {len(region_groups)}개의 지역별 CSV 파일로 나누어 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN(temp, humidity, wind 컬럼 결측치 처리) - (1)\n",
    "#### Elbow method 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 수치형만\n",
    "numeric_df = sample_df[numeric_cols].copy()\n",
    "original_nan_mask = numeric_df.isna()\n",
    "mask_candidates = ~original_nan_mask\n",
    "random_mask = np.random.rand(*numeric_df.shape) < 0.1\n",
    "mask = random_mask & mask_candidates\n",
    "\n",
    "# 인위적 결측 생성\n",
    "masked_df = numeric_df.copy()\n",
    "masked_df[mask] = np.nan\n",
    "\n",
    "# Elbow loop\n",
    "rmse_list = []\n",
    "for k in range(1, 16):\n",
    "    imputer = KNNImputer(n_neighbors=k)\n",
    "    imputed = imputer.fit_transform(masked_df)\n",
    "\n",
    "    # 1. flatten된 mask 위치에서의 값 추출\n",
    "    true_vals = numeric_df.values[mask]  # 원래 값\n",
    "    pred_vals = imputed[mask]  # 복원된 값\n",
    "\n",
    "    # 2. 길이 확인 (디버깅용)\n",
    "    print(f\"k={k} | true={true_vals.shape}, pred={pred_vals.shape}\")\n",
    "\n",
    "    # 3. RMSE 계산 및 저장\n",
    "    rmse = np.sqrt(mean_squared_error(true_vals, pred_vals))\n",
    "    rmse_list.append(rmse)\n",
    "\n",
    "# Plot\n",
    "plt.plot(range(1, 16), rmse_list, marker=\"o\")\n",
    "plt.xlabel(\"n_neighbors\")\n",
    "plt.ylabel(\"RMSE\")\n",
    "plt.title(\"Elbow Method for Optimal n_neighbors\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN(temp, humidity, wind 컬럼 결측치 처리) - (2)\n",
    "#### KNN imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결측치 처리 방법 1. KNN Imputation\n",
    "### KNN Imputation: 결측치를 가장 유사한 K개의 이웃값을 평균/가중 평균하여 채우는 방식\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# KNN 전용 데이터프레임 df_KNN 생성\n",
    "df_KNN = df_data_all\n",
    "\n",
    "# 수치형 변수만 대상으로 KNN Imputation 수행\n",
    "# KNNImputer는 수치형만 처리 가능하므로 문자열/범주형 제외\n",
    "num_cols = df_KNN.select_dtypes(include=[np.number]).columns\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# 결과 저장용 리스트\n",
    "imputed_groups = []\n",
    "\n",
    "# 수치형 컬럼\n",
    "num_cols = df_KNN.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "# 지역별로 나누어 KNN Imputer 적용\n",
    "imputed_groups = []\n",
    "for region_name, group in df_KNN.groupby(\"region\"):\n",
    "    group_copy = group.copy()\n",
    "\n",
    "    # 해당 그룹 내 수치형 컬럼에 대해서만 KNN 적용\n",
    "    # optimal k = 4 (elbow method 참조)\n",
    "    imputer = KNNImputer(n_neighbors=4)\n",
    "    group_copy[num_cols] = imputer.fit_transform(group_copy[num_cols])\n",
    "\n",
    "    imputed_groups.append(group_copy)\n",
    "\n",
    "# 병합\n",
    "df_KNN_imputed = pd.concat(imputed_groups).sort_index()\n",
    "\n",
    "# 결과 확인\n",
    "print(\"✅ 지역별 KNN Imputation 완료!\")\n",
    "print(\"📦 최종 데이터 크기:\", df_KNN_imputed.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wildfire",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
